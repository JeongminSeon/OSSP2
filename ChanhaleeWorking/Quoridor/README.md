# 정우철 진행사항

## 1. DQN으로의 학습
- 쿼리도를 대상으로 직접 학습하기 전 틱택토를 대상으로 DQN을 이용하여 학습을 진행하였고 만족 할만한 결과를 얻었다. 이를 Quoridor에 적용하였다.

### 1-1. Agent
- DQN을 이용하여 학습을 진행하였다. 이를 위해 Agent를 구현하였다. Agent는 다음과 같은 구조를 가지고 있다.

## 시행착오와 해결
- 쿼리도를 대상으로 직접 학습하기 전 틱택토를 대상으로 DQN을 이용하여 학습을 진행하였고 만족 할만한 결과를 보였으나, 쿼리도를 대상으로 학습을 진행하였을 때는 많은 경우의 수에 비하여 Train이 됨에 따라 상황에 따라 선택하는 Action의 값이 비교적 한정적으로만 선택되는 현상이 발생하였다. Epsilon 값을 설정해 줌에도 불구하고, Epsilon 값이 충분히 작아진 후에는, 상황에 대해 늘 선택하던 Action만 선택하였다. 초반의 특정한 경우에 대해서만 학습을 진행하다 보니, 만약 초반에 방문하지 못한 선택을 하게 되면, 그 이후에는 자주 선택되던 경우보다 좋지 않은 행동을 할 가능성이 높다.

### 궁금한 부분
- 따라서, 결정론적 정책을 반환하는 DQN이 아닌, 확률적 정책을 반환하는 A2C를 사용하여 학습을 진행하였다.

### 해결 방법
- 현재 

### 결과
1. 

### 추가로 고민할 점
- 1