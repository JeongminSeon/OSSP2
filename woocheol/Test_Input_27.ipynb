{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocE7fnICOgDg","executionInfo":{"status":"ok","timestamp":1668415767542,"user_tz":-540,"elapsed":7386,"user":{"displayName":"정우철","userId":"04737966036227576030"}},"outputId":"8dab3c0e-1c3b-4902-bc24-2c98293cd913"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"KHfI7gtXOUKK","executionInfo":{"status":"ok","timestamp":1668417117893,"user_tz":-540,"elapsed":466,"user":{"displayName":"정우철","userId":"04737966036227576030"}}},"outputs":[],"source":["class TicTacToe():\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.board = [0, 0, 0, 0, 0, 0, 0, 0, 0] * 3\n","        self.turn = 1\n","        self.gameover = False\n","        return self.board\n","\n","    def get_state(self):\n","        if self.turn == 1:\n","            return self.board\n","        else:\n","            new_board = []\n","            for i in range(9):\n","                if self.board[i] == 1:\n","                    new_board.append(2)\n","                elif self.board[i] == 2:\n","                    new_board.append(1)\n","                else:\n","                    new_board.append(0)\n","            return new_board * 3\n","\n","    def step(self, action):\n","        # 턴 미리 바꿔주기\n","        self.turn = 3 - self.turn\n","\n","        # 불가능한 액션\n","        if self.board[action] != 0:\n","            return self.get_state(), -10, True\n","        self.board[action] = self.turn\n","\n","        # 승리하는 수를 놓았는지\n","        if self.check_reward():\n","            return self.get_state(), 2, True\n","\n","        # 패배하는 수를 놓았는지\n","        if self.check_defeatable():\n","            return self.get_state(), -1, False\n","\n","        # 무승부인지\n","        if self.check_isfull():\n","            return self.get_state(), 1, True\n","\n","        return self.get_state(), 0, False\n","\n","    def check_isfull(self):\n","        for i in range(9):\n","            if self.board[i] == 0:\n","                return False\n","        return True\n","\n","    def check_defeatable(self):\n","        for i in range(3):\n","            if (self.board[i * 3] == self.board[i * 3 + 1] == (3 - self.turn) and self.board[i * 3 + 2] == 0) or\\\n","                (self.board[i * 3] == self.board[i * 3 + 2] == (3 - self.turn) and self.board[i * 3 + 1] == 0) or \\\n","                    (self.board[i * 3 + 1] == self.board[i * 3 + 2] == (3 - self.turn) and self.board[i * 3] == 0):\n","                return True\n","            if (self.board[i] == self.board[i + 3] == (3 - self.turn) and self.board[i + 6] == 0) or \\\n","                (self.board[i] == self.board[i + 6] == (3 - self.turn) and self.board[i + 3] == 0) or \\\n","                    (self.board[i + 3] == self.board[i + 6] == (3 - self.turn) and self.board[i] == 0):\n","                return True\n","        if (self.board[0] == self.board[4] == (3 - self.turn) and self.board[8] == 0) or \\\n","            (self.board[0] == self.board[8] == (3 - self.turn) and self.board[4] == 0) or \\\n","                (self.board[4] == self.board[8] == (3 - self.turn) and self.board[0] == 0):\n","            return True\n","        if (self.board[2] == self.board[4] == (3 - self.turn) and self.board[6] == 0) or \\\n","            (self.board[2] == self.board[6] == (3 - self.turn) and self.board[4] == 0) or \\\n","                (self.board[4] == self.board[6] == (3 - self.turn) and self.board[2] == 0):\n","            return True\n","        return False\n","\n","    def available_actions(self):\n","        actions = []\n","        for i in range(9):\n","            if self.board[i] == 0:\n","                actions.append(i)\n","        return actions\n","\n","    def check_reward(self):\n","        for i in range(3):\n","            if self.board[i * 3] == self.board[i * 3 + 1] == self.board[i * 3 + 2] != 0:\n","                return 1\n","            if self.board[i] == self.board[i + 3] == self.board[i + 6] != 0:\n","                return 1\n","        if self.board[0] == self.board[4] == self.board[8] != 0:\n","            return 1\n","        if self.board[2] == self.board[4] == self.board[6] != 0:\n","            return 1\n","        return 0\n","\n","    def render(self):\n","        print(self.turn)\n","        for i in range(3):\n","            for j in range(3):\n","                if self.board[i * 3 + j] == 1:\n","                    print('O', end='')\n","                elif self.board[i * 3 + j] == 2:\n","                    print('X', end='')\n","                else:\n","                    print(' ', end='')\n","            print()\n","        print()\n"]},{"cell_type":"code","source":["import collections\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","\n","# Hyperparameters\n","learning_rate = 0.00025\n","gamma = 1\n","buffer_limit = 100000\n","batch_size = 32\n","\n","loss_sum = 0\n","\n","\n","class ReplayBuffer():\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","\n","    def put(self, transition):\n","        self.buffer.append(transition)\n","\n","    def sample(self, n):\n","        mini_batch = random.sample(self.buffer, n)\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","\n","        for transition in mini_batch:\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","\n","        return torch.tensor(s_lst, dtype=torch.float).cuda(), torch.tensor(a_lst).cuda(), \\\n","            torch.tensor(r_lst).cuda(), torch.tensor(s_prime_lst, dtype=torch.float).cuda(), \\\n","            torch.tensor(done_mask_lst).cuda()\n","\n","    def size(self):\n","        return len(self.buffer)\n","\n","\n","class Qnet(nn.Module):\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","        self.fc1 = nn.Linear(9*3, 256)\n","        self.fc2 = nn.Linear(256, 256)\n","        self.fc3 = nn.Linear(256, 256)\n","        self.fc4 = nn.Linear(256, 256)\n","        self.fc5 = nn.Linear(256, 9)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.relu(self.fc4(x))\n","        x = self.fc5(x)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):\n","        out = self.forward(obs)\n","        coin = random.random()\n","        if coin < epsilon:\n","            return random.randint(0, 8)\n","        else:\n","            return out.argmax().item()\n","\n","\n","def train(q, q_target, memory, optimizer):\n","    global loss_sum\n","    for i in range(10):\n","        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n","\n","        q_out = q(s)\n","        q_a = q_out.gather(1, a)\n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r + gamma * max_q_prime * done_mask\n","        loss = F.smooth_l1_loss(q_a, target)\n","        loss_sum += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","def main():\n","    env = TicTacToe()\n","    q = Qnet().cuda()\n","    q_target = Qnet().cuda()\n","    \n","    # 불러오기\n","    # q.load_state_dict(torch.load('/gdrive/MyDrive/ossp2/DQN1.pth'))\n","    \n","    q_target.load_state_dict(q.state_dict())\n","    memory = ReplayBuffer()\n","\n","    print_interval = 20\n","    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","\n","    for n_epi in range(20000):\n","        epsilon = max(0.01, 0.2 - 0.01*(n_epi/200))\n","        s = env.reset()\n","        done = False\n","\n","        while not done:\n","            # env.render()\n","            a = q.sample_action(torch.from_numpy(np.array(s)).float().cuda(), epsilon)\n","\n","            available_actions = env.available_actions()\n","            if a not in available_actions:\n","                s_prime, r, done = env.step(random.choice(available_actions))\n","            else:\n","                s_prime, r, done = env.step(a)\n","            \n","            done_mask = 0.0 if done else 1.0\n","            memory.put((s, a, r/100.0, s_prime, done_mask))\n","            s = s_prime\n","\n","            if done:\n","                break\n","\n","        if memory.size() > 2000:\n","            train(q, q_target, memory, optimizer)\n","\n","        if n_epi % print_interval == 0 and n_epi != 0:\n","            q_target.load_state_dict(q.state_dict())\n","            print(n_epi, \"loss sum: \",loss_sum)\n","\n","    torch.save(q.state_dict(), '/gdrive/MyDrive/DQN1.pth')\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NP6A3zTQOYhW","outputId":"5be0f829-76fe-401f-f2a5-bea7f314f672"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20 loss sum:  0\n","40 loss sum:  0\n","60 loss sum:  0\n","80 loss sum:  0\n","100 loss sum:  0\n","120 loss sum:  0\n","140 loss sum:  0\n","160 loss sum:  0\n","180 loss sum:  0\n","200 loss sum:  0\n","220 loss sum:  0\n","240 loss sum:  0\n","260 loss sum:  0.014479881792794913\n","280 loss sum:  0.03950716927101894\n","300 loss sum:  0.05791999700704764\n","320 loss sum:  0.0784501064408687\n","340 loss sum:  0.0987197378926794\n","360 loss sum:  0.11764931467405404\n","380 loss sum:  0.13678867762064328\n","400 loss sum:  0.15356049586989684\n","420 loss sum:  0.17127715349124628\n","440 loss sum:  0.1871655227168958\n","460 loss sum:  0.204757799003346\n","480 loss sum:  0.21994588607594778\n","500 loss sum:  0.23451442164878245\n","520 loss sum:  0.24930027136360877\n","540 loss sum:  0.26158529751046444\n","560 loss sum:  0.2759781268414372\n","580 loss sum:  0.2884376626516314\n","600 loss sum:  0.30142019568666\n","620 loss sum:  0.31134146262866125\n","640 loss sum:  0.3223078941982749\n","660 loss sum:  0.33151479156094865\n","680 loss sum:  0.3415781360681649\n","700 loss sum:  0.3508474117397782\n","720 loss sum:  0.3604926768630321\n","740 loss sum:  0.370093167343839\n","760 loss sum:  0.37886644345144305\n","780 loss sum:  0.3866111353336237\n","800 loss sum:  0.39390045881373226\n","820 loss sum:  0.40144605905152275\n","840 loss sum:  0.4094506016535888\n","860 loss sum:  0.4167585330296788\n","880 loss sum:  0.42358399758722953\n","900 loss sum:  0.4302102883666521\n","920 loss sum:  0.4368095251666091\n","940 loss sum:  0.44372278558603284\n","960 loss sum:  0.4501560434491694\n","980 loss sum:  0.456248782824332\n","1000 loss sum:  0.4620178203103933\n","1020 loss sum:  0.4670355876760368\n","1040 loss sum:  0.47194888298417936\n","1060 loss sum:  0.47671964552273494\n","1080 loss sum:  0.4815919580919399\n","1100 loss sum:  0.48604030009801136\n","1120 loss sum:  0.4907510669613657\n","1140 loss sum:  0.49504445553566256\n","1160 loss sum:  0.49913571995284656\n","1180 loss sum:  0.5034037527389046\n","1200 loss sum:  0.5071653625946055\n","1220 loss sum:  0.5111521160288248\n","1240 loss sum:  0.5149649574641444\n","1260 loss sum:  0.5184215672693426\n","1280 loss sum:  0.5217657308421622\n","1300 loss sum:  0.524828691485709\n","1320 loss sum:  0.5280366043216418\n","1340 loss sum:  0.5307309180398079\n","1360 loss sum:  0.5334405983389843\n","1380 loss sum:  0.5359575980328373\n","1400 loss sum:  0.5383561857634049\n","1420 loss sum:  0.5408466398776\n","1440 loss sum:  0.5431462094938979\n","1460 loss sum:  0.5453964724404159\n","1480 loss sum:  0.5475146496844445\n","1500 loss sum:  0.5496196687479369\n","1520 loss sum:  0.5515517202970841\n","1540 loss sum:  0.5534546451276583\n","1560 loss sum:  0.5552864445867272\n","1580 loss sum:  0.5572167577363416\n","1600 loss sum:  0.5588827216802201\n","1620 loss sum:  0.5605579335301627\n","1640 loss sum:  0.5620889334052208\n","1660 loss sum:  0.5637014344979434\n","1680 loss sum:  0.5652001784142158\n","1700 loss sum:  0.5668896076467718\n","1720 loss sum:  0.5684965617624584\n","1740 loss sum:  0.5700645956526387\n","1760 loss sum:  0.571605206238246\n","1780 loss sum:  0.5731449220255627\n","1800 loss sum:  0.5746538688644023\n","1820 loss sum:  0.5760577506983964\n","1840 loss sum:  0.5775148364335791\n","1860 loss sum:  0.5789976903098477\n","1880 loss sum:  0.5804585819427075\n","1900 loss sum:  0.5818515422068913\n","1920 loss sum:  0.583267674600279\n","1940 loss sum:  0.5846357824024153\n","1960 loss sum:  0.5860366981206653\n","1980 loss sum:  0.5874033088531405\n","2000 loss sum:  0.5887047909453713\n","2020 loss sum:  0.5900110856692891\n","2040 loss sum:  0.5913739789453984\n","2060 loss sum:  0.5927009158330065\n","2080 loss sum:  0.5940829043300937\n","2100 loss sum:  0.5955009212910909\n","2120 loss sum:  0.5967782211033636\n","2140 loss sum:  0.5980701800104953\n","2160 loss sum:  0.5993795168354836\n","2180 loss sum:  0.6006333698634307\n","2200 loss sum:  0.6018812626891759\n","2220 loss sum:  0.6031376410251141\n","2240 loss sum:  0.604482789504118\n","2260 loss sum:  0.6057027666245176\n","2280 loss sum:  0.6069758281848863\n","2300 loss sum:  0.6082591716344723\n","2320 loss sum:  0.6095089530538189\n","2340 loss sum:  0.6107733245287363\n","2360 loss sum:  0.6120052904921067\n","2380 loss sum:  0.6133140180879764\n","2400 loss sum:  0.6146731733128945\n","2420 loss sum:  0.6158412269874134\n","2440 loss sum:  0.617090667463799\n","2460 loss sum:  0.618333374093254\n","2480 loss sum:  0.6196212105426184\n"]}]}]}